{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "55162e313bbc4c31844871d9d5f36d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb942f75c3c141b78d3b05b07f32b847",
              "IPY_MODEL_1eac648057dd4ed4ac81e7a3b28e3eb4",
              "IPY_MODEL_abb4cae1be1346499c01e34feae14c5b"
            ],
            "layout": "IPY_MODEL_e50e89bf8efc4a1ca6e6b56476165f31"
          }
        },
        "cb942f75c3c141b78d3b05b07f32b847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f7f96b7a47b46c5a14f406bad111349",
            "placeholder": "​",
            "style": "IPY_MODEL_2723fabc708646f1ad50ebdbe1b3ce0a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "1eac648057dd4ed4ac81e7a3b28e3eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6dfab1bbc92458db5edf629e04b08b5",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b4df23e024840a18bc522d14b1d5efd",
            "value": 25
          }
        },
        "abb4cae1be1346499c01e34feae14c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74778158aaae43949fdf2abab04caed7",
            "placeholder": "​",
            "style": "IPY_MODEL_292e76ee6deb42fc80920f2081f0145a",
            "value": " 25.0/25.0 [00:00&lt;00:00, 3.23kB/s]"
          }
        },
        "e50e89bf8efc4a1ca6e6b56476165f31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f7f96b7a47b46c5a14f406bad111349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2723fabc708646f1ad50ebdbe1b3ce0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6dfab1bbc92458db5edf629e04b08b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b4df23e024840a18bc522d14b1d5efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74778158aaae43949fdf2abab04caed7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "292e76ee6deb42fc80920f2081f0145a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5529d2c4e04c46e59cf522d32e45a56e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b21a4d3f654409489a44a8930fc09fe",
              "IPY_MODEL_f87dbc4e83e545048243a34ded474edd",
              "IPY_MODEL_05c1217846734daf933a65256460aaf7"
            ],
            "layout": "IPY_MODEL_04557b7b990d426e90e91e2c2e8a8e2a"
          }
        },
        "4b21a4d3f654409489a44a8930fc09fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9696d14882043d6b51edf186b90c686",
            "placeholder": "​",
            "style": "IPY_MODEL_554df94793b04d3e8281585dea39c4de",
            "value": "vocab.json: 100%"
          }
        },
        "f87dbc4e83e545048243a34ded474edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d811bf524e74bb4a67c0c7864f6ba9a",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64d33321bbe94599965ccd706dcb314d",
            "value": 898823
          }
        },
        "05c1217846734daf933a65256460aaf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39dcd3007ea042f3a7ab5fe41e4a6b26",
            "placeholder": "​",
            "style": "IPY_MODEL_aba39d0f815c4c709ee48b146513517b",
            "value": " 899k/899k [00:00&lt;00:00, 3.59MB/s]"
          }
        },
        "04557b7b990d426e90e91e2c2e8a8e2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9696d14882043d6b51edf186b90c686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "554df94793b04d3e8281585dea39c4de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d811bf524e74bb4a67c0c7864f6ba9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64d33321bbe94599965ccd706dcb314d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39dcd3007ea042f3a7ab5fe41e4a6b26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aba39d0f815c4c709ee48b146513517b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2b8d60257924b84b90a7cf76fee33f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53bcb60601984fb189acf3f95b085b32",
              "IPY_MODEL_df7b84b022aa4263835af926e3750b1a",
              "IPY_MODEL_f51d5ddf66cf416cb6626ad2cfca1956"
            ],
            "layout": "IPY_MODEL_d0085869d42e4d77a1746b2ce957a3de"
          }
        },
        "53bcb60601984fb189acf3f95b085b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4919573273d44c048e78784ced7bbcff",
            "placeholder": "​",
            "style": "IPY_MODEL_7a6773bb6841408e9779462c99796e78",
            "value": "merges.txt: 100%"
          }
        },
        "df7b84b022aa4263835af926e3750b1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4b35e040ab740a398dcad44654b9d09",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_523da53c9eda4b36aed7a73b09c52eae",
            "value": 456318
          }
        },
        "f51d5ddf66cf416cb6626ad2cfca1956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10c4b533046d4d9d98e287a6eeda42dc",
            "placeholder": "​",
            "style": "IPY_MODEL_d6941456d6064bf5bc8b71d804737aec",
            "value": " 456k/456k [00:00&lt;00:00, 4.40MB/s]"
          }
        },
        "d0085869d42e4d77a1746b2ce957a3de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4919573273d44c048e78784ced7bbcff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a6773bb6841408e9779462c99796e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4b35e040ab740a398dcad44654b9d09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "523da53c9eda4b36aed7a73b09c52eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10c4b533046d4d9d98e287a6eeda42dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6941456d6064bf5bc8b71d804737aec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee112d9dadd04fd8a21cc84c3f0ba3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_140676055f9d41fda93049a6a7348ac9",
              "IPY_MODEL_1905c4241d1545a5baa9d92e1af268a0",
              "IPY_MODEL_87c3407e574a4664b8fa9b783253cfc7"
            ],
            "layout": "IPY_MODEL_273062ab733349878ab5fbff38aa401a"
          }
        },
        "140676055f9d41fda93049a6a7348ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c36a521cbd7e4f9088252ac7f0dd67c3",
            "placeholder": "​",
            "style": "IPY_MODEL_22ebd169362645dbb2507a848931483c",
            "value": "tokenizer.json: 100%"
          }
        },
        "1905c4241d1545a5baa9d92e1af268a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74ff0c2e9ab8436a85e334417abf2321",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d94b6581f974e269d0906b62cb1981c",
            "value": 1355863
          }
        },
        "87c3407e574a4664b8fa9b783253cfc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_debd272dcfa246aba6444a270429b3f3",
            "placeholder": "​",
            "style": "IPY_MODEL_321c2b0ee25a412cbfcb62cd0121a4dd",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 4.08MB/s]"
          }
        },
        "273062ab733349878ab5fbff38aa401a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c36a521cbd7e4f9088252ac7f0dd67c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22ebd169362645dbb2507a848931483c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74ff0c2e9ab8436a85e334417abf2321": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d94b6581f974e269d0906b62cb1981c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "debd272dcfa246aba6444a270429b3f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "321c2b0ee25a412cbfcb62cd0121a4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36c2bd5320e74958b3a106ca81c074c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0de331f5e0534ae28e46e1f86c32ec21",
              "IPY_MODEL_64d268b5fb214019838306a4c4603b3e",
              "IPY_MODEL_28fc9db13e0845cda2502b5b82f24948"
            ],
            "layout": "IPY_MODEL_4a17b2b704704d859375afe381e344c5"
          }
        },
        "0de331f5e0534ae28e46e1f86c32ec21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf2ea6ff33484fc7b65c4b87b724be14",
            "placeholder": "​",
            "style": "IPY_MODEL_8ad0c444f52041d481a9137aa2eb2220",
            "value": "config.json: 100%"
          }
        },
        "64d268b5fb214019838306a4c4603b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0a62cd63cc044bbb0ce73f1ebd8cfca",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0284a400fd044f1eb997b9b0a402d573",
            "value": 481
          }
        },
        "28fc9db13e0845cda2502b5b82f24948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04316ab004f94a448b2c4a75128709a0",
            "placeholder": "​",
            "style": "IPY_MODEL_f9fbea44e21c4bb9b4b0025b16943611",
            "value": " 481/481 [00:00&lt;00:00, 67.1kB/s]"
          }
        },
        "4a17b2b704704d859375afe381e344c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf2ea6ff33484fc7b65c4b87b724be14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ad0c444f52041d481a9137aa2eb2220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0a62cd63cc044bbb0ce73f1ebd8cfca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0284a400fd044f1eb997b9b0a402d573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04316ab004f94a448b2c4a75128709a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9fbea44e21c4bb9b4b0025b16943611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "351014138c34473fa7a90b61bd2ced63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab16ee27dd764529aec5e52fedef914b",
              "IPY_MODEL_bbce2dd1340541f280bf4112913d36cd",
              "IPY_MODEL_16c9289e60754f8dacaaceef7b415c1e"
            ],
            "layout": "IPY_MODEL_3ad88479dbab4e9eb8ae96fce8b13b47"
          }
        },
        "ab16ee27dd764529aec5e52fedef914b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afbfd83d52cb45b183652742ea81896c",
            "placeholder": "​",
            "style": "IPY_MODEL_43da409b927341e99d4e9a5f8a8c2b19",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "bbce2dd1340541f280bf4112913d36cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdab8c57a705456ca6c367d2a22ec965",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05a7e71f5a6e45218f24c370e21d715f",
            "value": 501200538
          }
        },
        "16c9289e60754f8dacaaceef7b415c1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99dbc73710224a32b5d12cea4bde77d2",
            "placeholder": "​",
            "style": "IPY_MODEL_9249312af8db4fbd822f08865757ca62",
            "value": " 501M/501M [00:04&lt;00:00, 215MB/s]"
          }
        },
        "3ad88479dbab4e9eb8ae96fce8b13b47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afbfd83d52cb45b183652742ea81896c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43da409b927341e99d4e9a5f8a8c2b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdab8c57a705456ca6c367d2a22ec965": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05a7e71f5a6e45218f24c370e21d715f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99dbc73710224a32b5d12cea4bde77d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9249312af8db4fbd822f08865757ca62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "run once"
      ],
      "metadata": {
        "id": "nmZ0Krz4BPdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.30.0 --no-deps #compatibility issues between tf and Transformers\n",
        "!pip install tokenizers==0.13.3 --no-build-isolation\n",
        "!pip install tensorflow==2.18.0 as\n",
        "!pip install --upgrade transformers tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JlI8xJqiVUb",
        "outputId": "4f1866bc-944b-4b97-f6ce-62f1a0598ca6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.30.0\n",
            "  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "Successfully installed transformers-4.30.0\n",
            "Collecting tokenizers==0.13.3\n",
            "  Downloading tokenizers-0.13.3.tar.gz (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.9/314.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting tensorflow==2.18.0\n",
            "  Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement as (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for as\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.30.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m153.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, transformers, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.30.0\n",
            "    Uninstalling transformers-4.30.0:\n",
            "      Successfully uninstalled transformers-4.30.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.19.0\n",
            "    Uninstalling tensorflow-2.19.0:\n",
            "      Successfully uninstalled tensorflow-2.19.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.20.0 tensorflow-2.20.0 transformers-4.57.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mJOp4hbi_wMV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Workaround for safetensors issue\n",
        "os.environ['SAFETENSORS_FAST_GPU'] = '1'\n",
        "model_name = 'roberta-base'\n",
        "print(f\"\\nLoading {model_name}...\")\n",
        "\n",
        "try:\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "    bert_model = TFRobertaModel.from_pretrained(model_name, from_pt=True)\n",
        "    print(\"\\nModel loaded successfully (from PyTorch checkpoint)\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nTrying alternative loading method...\")\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "    bert_model = TFRobertaModel.from_pretrained(model_name, use_safetensors=False)\n",
        "    print(\"\\nModel loaded successfully (without safetensors)\")\n",
        "print(f\"  Model: {model_name}\")\n",
        "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541,
          "referenced_widgets": [
            "55162e313bbc4c31844871d9d5f36d0d",
            "cb942f75c3c141b78d3b05b07f32b847",
            "1eac648057dd4ed4ac81e7a3b28e3eb4",
            "abb4cae1be1346499c01e34feae14c5b",
            "e50e89bf8efc4a1ca6e6b56476165f31",
            "0f7f96b7a47b46c5a14f406bad111349",
            "2723fabc708646f1ad50ebdbe1b3ce0a",
            "d6dfab1bbc92458db5edf629e04b08b5",
            "4b4df23e024840a18bc522d14b1d5efd",
            "74778158aaae43949fdf2abab04caed7",
            "292e76ee6deb42fc80920f2081f0145a",
            "5529d2c4e04c46e59cf522d32e45a56e",
            "4b21a4d3f654409489a44a8930fc09fe",
            "f87dbc4e83e545048243a34ded474edd",
            "05c1217846734daf933a65256460aaf7",
            "04557b7b990d426e90e91e2c2e8a8e2a",
            "a9696d14882043d6b51edf186b90c686",
            "554df94793b04d3e8281585dea39c4de",
            "5d811bf524e74bb4a67c0c7864f6ba9a",
            "64d33321bbe94599965ccd706dcb314d",
            "39dcd3007ea042f3a7ab5fe41e4a6b26",
            "aba39d0f815c4c709ee48b146513517b",
            "e2b8d60257924b84b90a7cf76fee33f9",
            "53bcb60601984fb189acf3f95b085b32",
            "df7b84b022aa4263835af926e3750b1a",
            "f51d5ddf66cf416cb6626ad2cfca1956",
            "d0085869d42e4d77a1746b2ce957a3de",
            "4919573273d44c048e78784ced7bbcff",
            "7a6773bb6841408e9779462c99796e78",
            "d4b35e040ab740a398dcad44654b9d09",
            "523da53c9eda4b36aed7a73b09c52eae",
            "10c4b533046d4d9d98e287a6eeda42dc",
            "d6941456d6064bf5bc8b71d804737aec",
            "ee112d9dadd04fd8a21cc84c3f0ba3c8",
            "140676055f9d41fda93049a6a7348ac9",
            "1905c4241d1545a5baa9d92e1af268a0",
            "87c3407e574a4664b8fa9b783253cfc7",
            "273062ab733349878ab5fbff38aa401a",
            "c36a521cbd7e4f9088252ac7f0dd67c3",
            "22ebd169362645dbb2507a848931483c",
            "74ff0c2e9ab8436a85e334417abf2321",
            "9d94b6581f974e269d0906b62cb1981c",
            "debd272dcfa246aba6444a270429b3f3",
            "321c2b0ee25a412cbfcb62cd0121a4dd",
            "36c2bd5320e74958b3a106ca81c074c1",
            "0de331f5e0534ae28e46e1f86c32ec21",
            "64d268b5fb214019838306a4c4603b3e",
            "28fc9db13e0845cda2502b5b82f24948",
            "4a17b2b704704d859375afe381e344c5",
            "cf2ea6ff33484fc7b65c4b87b724be14",
            "8ad0c444f52041d481a9137aa2eb2220",
            "f0a62cd63cc044bbb0ce73f1ebd8cfca",
            "0284a400fd044f1eb997b9b0a402d573",
            "04316ab004f94a448b2c4a75128709a0",
            "f9fbea44e21c4bb9b4b0025b16943611",
            "351014138c34473fa7a90b61bd2ced63",
            "ab16ee27dd764529aec5e52fedef914b",
            "bbce2dd1340541f280bf4112913d36cd",
            "16c9289e60754f8dacaaceef7b415c1e",
            "3ad88479dbab4e9eb8ae96fce8b13b47",
            "afbfd83d52cb45b183652742ea81896c",
            "43da409b927341e99d4e9a5f8a8c2b19",
            "bdab8c57a705456ca6c367d2a22ec965",
            "05a7e71f5a6e45218f24c370e21d715f",
            "99dbc73710224a32b5d12cea4bde77d2",
            "9249312af8db4fbd822f08865757ca62"
          ]
        },
        "id": "OF7L1f4Fx0gj",
        "outputId": "0030919f-e1b7-4d2c-da1c-1b7c2b1807ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading roberta-base...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55162e313bbc4c31844871d9d5f36d0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5529d2c4e04c46e59cf522d32e45a56e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2b8d60257924b84b90a7cf76fee33f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee112d9dadd04fd8a21cc84c3f0ba3c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36c2bd5320e74958b3a106ca81c074c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "351014138c34473fa7a90b61bd2ced63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model loaded successfully (from PyTorch checkpoint)\n",
            "  Model: roberta-base\n",
            "  Vocab size: 50,265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Upload\n",
        "unzip them (run once)"
      ],
      "metadata": {
        "id": "ehHKF3k2B2dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# load datasets\n",
        "amazon_zip = '/content/drive/MyDrive/571_Proj/amazon_cleaned/finalcombined_cleaned.csv.zip'\n",
        "yelp_zip = '/content/drive/MyDrive/571_Proj/yelp_cleaned/yelpchi_merged_restaurant_hotel.csv.zip'\n",
        "\n",
        "with zipfile.ZipFile(amazon_zip, 'r') as zip_ref:\n",
        "  zip_ref.extractall('content/data/')\n",
        "  amazon_files = zip_ref.namelist()\n",
        "\n",
        "with zipfile.ZipFile(yelp_zip, 'r') as zip_ref:\n",
        "  zip_ref.extractall('content/data/')\n",
        "  yelp_files = zip_ref.namelist()\n",
        "\n",
        "print(\"Datasets extracted\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_tqf3moOB5HC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5661c2-77d9-49a7-e4e7-b5b5269675b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Datasets extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "load them as csv's"
      ],
      "metadata": {
        "id": "h0I3yqLAEbGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amazon = pd.read_csv('/content/content/data/finalcombined_cleaned.csv')\n",
        "yelp = pd.read_csv('/content/content/data/yelpchi_merged_restaurant_hotel.csv')"
      ],
      "metadata": {
        "id": "w8-8WVg7Ec6C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "display data and size"
      ],
      "metadata": {
        "id": "F8BWnvpEFaY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AMAZON\\n\")\n",
        "print(f\"\\nAmazon columns: {amazon.columns.tolist()}\")\n",
        "print(amazon.head(5))\n",
        "print(amazon.shape)\n",
        "\n",
        "print(\"YELP\\n\")\n",
        "print(f\"\\nYelp columns: {yelp.columns.tolist()}\")\n",
        "print(yelp.head(5))\n",
        "print(yelp.shape)\n"
      ],
      "metadata": {
        "id": "9RL2hK_TFdE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "053024f4-ae9b-4319-e04a-04e68f932438"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AMAZON\n",
            "\n",
            "\n",
            "Amazon columns: ['review_id', 'reviewer_id', 'product_id', 'text', 'title', 'rating', 'unixReviewTime', 'label', 'category', 'helpful_votes', 'total_votes', 'helpfulness_ratio']\n",
            "                  review_id     reviewer_id  product_id  \\\n",
            "0  5a132224741a2384e81d941a  A2GDZUHVGLNDKB  B008BUVXX4   \n",
            "1  5a132269741a2384e835d60b   AR0I45CCR5HKL  B00HLUAO3U   \n",
            "2  5a132209741a2384e8142d5f  A2N4C3YLXG0LO0  B005SUHPO6   \n",
            "3  5a132216741a2384e818b740  A1K6EYQNQ1JEAB  B0077RO08C   \n",
            "4  5a1321e0741a2384e80658c2  A2TCV4RZUK2LL6  B001YPYYJ8   \n",
            "\n",
            "                                                text                    title  \\\n",
            "0  this can be used 2 ways. its durable and looks...               great case   \n",
            "1  excelent item.recomend agood battery compact a...       good cover battery   \n",
            "2  for the price of this case, its definitely wor...              great value   \n",
            "3  i bought this because i work 12 hours a day an...  perfect for work or car   \n",
            "4  this phone works well, has good sound quality,...      sturdy little phone   \n",
            "\n",
            "   rating  unixReviewTime  label                     category  helpful_votes  \\\n",
            "0     5.0      1389312000    1.0  Cell_Phones_and_Accessories              0   \n",
            "1     4.0      1394496000    1.0  Cell_Phones_and_Accessories              0   \n",
            "2     4.0      1355097600    1.0  Cell_Phones_and_Accessories              0   \n",
            "3     5.0      1365120000    1.0  Cell_Phones_and_Accessories              1   \n",
            "4     4.0      1395532800    1.0  Cell_Phones_and_Accessories              0   \n",
            "\n",
            "   total_votes  helpfulness_ratio  \n",
            "0            0                0.0  \n",
            "1            0                0.0  \n",
            "2            0                0.0  \n",
            "3            2                0.5  \n",
            "4            0                0.0  \n",
            "(4876713, 12)\n",
            "YELP\n",
            "\n",
            "\n",
            "Yelp columns: ['review_id', 'user_id', 'business_id', 'date', 'stars', 'label', 'review_text', 'domain']\n",
            "                review_id                 user_id             business_id  \\\n",
            "0  GtwU21YOQn-wf4vWRUIx6w  bNYesZ944s6IJVowOnB0iA  pbEiXam9YJL3neCYHGwLUA   \n",
            "1                 0LpVTc3  TRKxLC3y-ZvP45e5iilMtw  pbEiXam9YJL3neCYHGwLUA   \n",
            "2           tljtLzf68Fkwf  0EMm8umAqXZzyhxNpL4M9g  pbEiXam9YJL3neCYHGwLUA   \n",
            "3                     iSN  DlwexC7z88ymAzu45skODw  pbEiXam9YJL3neCYHGwLUA   \n",
            "4                  Jmwrh7  kW2dk1CWihmh3g7k9N2G8A  pbEiXam9YJL3neCYHGwLUA   \n",
            "\n",
            "        date  stars  label                                        review_text  \\\n",
            "0  9/22/2012      5      0  Unlike Next, which we'd eaten at the previous ...   \n",
            "1  9/22/2012      5      0  Probably one of the best meals I've had ever. ...   \n",
            "2  9/19/2012      3      0  Service was impeccable. Experience and present...   \n",
            "3   9/6/2012      3      0  The problem with places like this, given the e...   \n",
            "4   9/9/2012      5      0  I have no idea how to write my review - dining...   \n",
            "\n",
            "       domain  \n",
            "0  restaurant  \n",
            "1  restaurant  \n",
            "2  restaurant  \n",
            "3  restaurant  \n",
            "4  restaurant  \n",
            "(67395, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning\n",
        "Original Amazon dataset has ~5million rows which is way too big, so we sampled it down."
      ],
      "metadata": {
        "id": "yVW0Mjq_GWUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can't run this more than once"
      ],
      "metadata": {
        "id": "fe93c7skJj8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we want our amazon dataset to have balanced sampling, and similar to\n",
        "# the 85/15 real/fake distribution of the yelp dataset\n",
        "\n",
        "amazon_fake = amazon[amazon['label'] == 1.0]\n",
        "amazon_real = amazon[amazon['label'] == 0.0]\n",
        "\n",
        "sample_size = 75000\n",
        "real = 0.70\n",
        "fake = 0.30\n",
        "\n",
        "n_real = int(sample_size * real) #~280k real\n",
        "n_fake = int(sample_size * fake) #~120k fake\n",
        "\n",
        "# sample from each class\n",
        "real_sample = amazon_real.sample(n=n_real, random_state=42)\n",
        "fake_sample = amazon_fake.sample(n=n_fake, random_state=42)\n",
        "\n",
        "# combine and shuffle\n",
        "amazon_sample = pd.concat([real_sample, fake_sample])\n",
        "amazon_sample = amazon_sample.sample(frac=1, random_state=42) #shuffle\n"
      ],
      "metadata": {
        "id": "ZVtIRfmPF_LG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#also need to rename 'text' to 'content' to match Bai(2024) code.\n",
        "amazon = amazon_sample.rename(columns={'text':'content'})\n",
        "\n",
        "# only need content and label\n",
        "amazon = amazon[['content', 'label']].copy()\n",
        "\n",
        "#verify it worked\n",
        "print(amazon.shape)\n",
        "print(amazon.columns.tolist())\n",
        "\n",
        "#the yelp data also needs a rename\n",
        "yelp = yelp.rename(columns={'review_text':'content'})\n",
        "yelp = yelp[['content', 'label']].copy()\n",
        "\n",
        "#make sure the content (review) is a string\n",
        "amazon['content'] = amazon['content'].astype(str)\n",
        "yelp['content'] = yelp['content'].astype(str)\n",
        "\n",
        "#remove nulls if any\n",
        "amazon = amazon[amazon['content'].notna()]\n",
        "yelp = yelp[yelp['content'].notna()]\n",
        "\n",
        "#verify\n",
        "print(yelp.shape)\n",
        "print(yelp.columns.tolist())"
      ],
      "metadata": {
        "id": "ZWECJlXVJhdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4a802d-b1e2-419e-a834-3bf0e3a5648f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(75000, 2)\n",
            "['content', 'label']\n",
            "(67395, 2)\n",
            "['content', 'label']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're minimizing the amount of memory usage for faster processing while still having large enough datasets for good results."
      ],
      "metadata": {
        "id": "WsqUW5KmKCdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Val/Test Split\n",
        "Amazon"
      ],
      "metadata": {
        "id": "T6Zp5XtlKTeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting 70/15/15\n",
        "train, remaining = train_test_split(\n",
        "    amazon,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=amazon['label']\n",
        ")\n",
        "\n",
        "valid, test = train_test_split(\n",
        "    remaining,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=remaining['label']\n",
        ")\n",
        "\n",
        "print(\"train \", train.shape, \" valid \", valid.shape, \" test \", test.shape)\n",
        "\n",
        "#yelp can just stay as yelp since the whole thing is a test set\n",
        "#printing yelp test just to compare to amazon for sizes\n",
        "\n",
        "print(\"yelp test\", yelp.shape)\n",
        "\n",
        "# we also want label distributions to ensure there's an evenish amount\n",
        "# of fake and real reviews in each set\n",
        "print(f\"train: {train['label'].value_counts().to_dict()}\")\n",
        "print(f\"valid: {valid['label'].value_counts().to_dict()}\")\n",
        "print(f\"test: {test['label'].value_counts().to_dict()}\")\n",
        "print(f\"yelp-test: {yelp['label'].value_counts().to_dict()}\")"
      ],
      "metadata": {
        "id": "CD-V-UW5J0l2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ac7a5d-e8e6-4efb-d976-c0c5ac6b11c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train  (52500, 2)  valid  (11250, 2)  test  (11250, 2)\n",
            "yelp test (67395, 2)\n",
            "train: {0.0: 36750, 1.0: 15750}\n",
            "valid: {0.0: 7875, 1.0: 3375}\n",
            "test: {0.0: 7875, 1.0: 3375}\n",
            "yelp-test: {0: 58476, 1: 8919}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a good distribution, with Amazon 70% real ad 30% fake, and Yelp 87% real and 13% fake. To change the distribution of the yelp reviews we'd have to decrease the sample size which is a larger drawback than a slightly more even distribution of fake and real."
      ],
      "metadata": {
        "id": "Zlp2-UwTSNpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Encoding"
      ],
      "metadata": {
        "id": "vOzasIW4X9Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is adapted from Bai(2024) RoBERTa.py lines 65 - 85\n",
        "\n",
        "#init encoders\n",
        "le = LabelEncoder()\n",
        "oh = OneHotEncoder()\n",
        "\n",
        "#Amazon labels\n",
        "train_labels = le.fit_transform(train['label'].values).reshape(-1,1)\n",
        "valid_labels = le.transform(valid['label'].values).reshape(-1,1)\n",
        "test_labels = le.transform(test['label'].values).reshape(-1,1)\n",
        "\n",
        "#One Hot Encoder\n",
        "train_labels = oh.fit_transform(train_labels).toarray()\n",
        "valid_labels = oh.transform(valid_labels).toarray()\n",
        "test_labels = oh.transform(test_labels).toarray()\n",
        "\n",
        "# Yelp labels\n",
        "yelp_labels = le.transform(yelp['label'].values).reshape(-1,1)\n",
        "yelp_labels = oh.transform(yelp_labels).toarray()\n",
        "\n",
        "print(f\"label 0.0 becomes {oh.transform([[0]]).toarray()}\")\n",
        "print(f\"label 1.0 becomes {oh.transform([[1]]).toarray()}\")\n",
        "\n",
        "print(train_labels[:3]) #sample to ensure it's looking right\n"
      ],
      "metadata": {
        "id": "Zn3OHDk-WfQB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785546f4-0aed-489a-d1af-be854af5f02b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label 0.0 becomes [[1. 0.]]\n",
            "label 1.0 becomes [[0. 1.]]\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RoBERTa Time"
      ],
      "metadata": {
        "id": "q3euphtjb65y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code citation: https://github.com/bailuweixi2/RoBERTa-MultiFeature-Model/blob/main/Code/Feature%20Extraction/RoBERTa.py"
      ],
      "metadata": {
        "id": "bEK-YejicIdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "\n",
        "# load roberta model and tokenizer\n",
        "model_name = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "bert_model = TFRobertaModel.from_pretrained(\n",
        "    model_name,\n",
        "    from_pt=True #force load from pytorch\n",
        ")\n",
        "\n",
        "#verify GPU being used\n",
        "gpus = tensorflow.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  print(\"gpu good\")\n",
        "else:\n",
        "  print(\"not using gpu btw\")\n"
      ],
      "metadata": {
        "id": "Pm-OMDZbhWzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd54164-8b72-49cf-fe33-cc6af672da8d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output is good"
      ],
      "metadata": {
        "id": "Yh4gah0W1ghv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m5CATNL2tMAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Amazon - Local\n",
        "Had to separate because it would crash on saving outputs.\n",
        "Also had to use chunking for local because it kept timing out."
      ],
      "metadata": {
        "id": "4jGMnByJyUWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Extracting Amazon TRAIN - Local chunked\")\n",
        "\n",
        "batch_size = 32\n",
        "max_length = 128\n",
        "CHUNK_SIZE = 20000\n",
        "\n",
        "def encode_and_convert_to_tensors(texts):\n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    return encoded\n",
        "\n",
        "train_texts = train['content'].tolist()\n",
        "num_train_examples = len(train_texts)\n",
        "\n",
        "train_all_outputs_0 = []\n",
        "chunk_num = 0\n",
        "samples_in_chunk = 0\n",
        "\n",
        "for i in range(0, num_train_examples, batch_size):\n",
        "    train_batch_texts = train_texts[i:i + batch_size]\n",
        "    train_batch_inputs = encode_and_convert_to_tensors(train_batch_texts)\n",
        "    train_batch_outputs = bert_model(train_batch_inputs)\n",
        "\n",
        "    train_all_outputs_0.append(train_batch_outputs[0].numpy())\n",
        "    samples_in_chunk += len(train_batch_texts)\n",
        "\n",
        "    del train_batch_inputs, train_batch_outputs\n",
        "\n",
        "    if samples_in_chunk >= CHUNK_SIZE or i + batch_size >= num_train_examples:\n",
        "        chunk_data = np.concatenate(train_all_outputs_0, axis=0)\n",
        "        np.save(f'/content/train_local_chunk_{chunk_num}.npy', chunk_data)\n",
        "\n",
        "        progress = ((i + batch_size) / num_train_examples) * 100\n",
        "        print(f\"  {progress:.1f}% - Chunk {chunk_num} saved\")\n",
        "\n",
        "        del train_all_outputs_0, chunk_data\n",
        "        train_all_outputs_0 = []\n",
        "        chunk_num += 1\n",
        "        samples_in_chunk = 0\n",
        "        gc.collect()\n",
        "\n",
        "del train_texts\n",
        "print(\"Complete\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6km7GSbqaHKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e02c814a-bdd7-4ad0-f123-779baa47407f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Amazon TRAIN - Local chunked\n",
            "  38.1% - Chunk 0 saved\n",
            "  76.2% - Chunk 1 saved\n",
            "  100.0% - Chunk 2 saved\n",
            "Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Amazon - Global"
      ],
      "metadata": {
        "id": "ERW5kJc346p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Extracting Amazon TRAIN - Global\")\n",
        "\n",
        "batch_size = 32\n",
        "max_length = 128\n",
        "\n",
        "def encode_and_convert_to_tensors(texts):\n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    return encoded\n",
        "\n",
        "train_texts = train['content'].tolist()\n",
        "num_train_examples = len(train_texts)\n",
        "\n",
        "train_all_outputs_1 = []\n",
        "\n",
        "for i in range(0, num_train_examples, batch_size):\n",
        "    train_batch_texts = train_texts[i:i + batch_size]\n",
        "    train_batch_inputs = encode_and_convert_to_tensors(train_batch_texts)\n",
        "    train_batch_outputs = bert_model(train_batch_inputs)\n",
        "\n",
        "    train_all_outputs_1.append(train_batch_outputs[1].numpy())\n",
        "\n",
        "    del train_batch_inputs, train_batch_outputs\n",
        "\n",
        "    if i % (batch_size * 50) == 0:\n",
        "        gc.collect()\n",
        "        progress = (i / num_train_examples) * 100\n",
        "        print(f\"  {progress:.1f}%\")\n",
        "\n",
        "train_global_output = np.concatenate(train_all_outputs_1, axis=0)\n",
        "\n",
        "print(\"Saving...\")\n",
        "np.save('/content/train_global_output.npy', train_global_output)\n",
        "\n",
        "gc.collect()\n",
        "print(\"Complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0KG9UQm49VQ",
        "outputId": "9d2962a7-a357-476e-bf6d-7df49d86b3f1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Amazon TRAIN - Global\n",
            "  0.0%\n",
            "  3.0%\n",
            "  6.1%\n",
            "  9.1%\n",
            "  12.2%\n",
            "  15.2%\n",
            "  18.3%\n",
            "  21.3%\n",
            "  24.4%\n",
            "  27.4%\n",
            "  30.5%\n",
            "  33.5%\n",
            "  36.6%\n",
            "  39.6%\n",
            "  42.7%\n",
            "  45.7%\n",
            "  48.8%\n",
            "  51.8%\n",
            "  54.9%\n",
            "  57.9%\n",
            "  61.0%\n",
            "  64.0%\n",
            "  67.0%\n",
            "  70.1%\n",
            "  73.1%\n",
            "  76.2%\n",
            "  79.2%\n",
            "  82.3%\n",
            "  85.3%\n",
            "  88.4%\n",
            "  91.4%\n",
            "  94.5%\n",
            "  97.5%\n",
            "Saving...\n",
            "Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Had to do concatention separately since it kept timing out"
      ],
      "metadata": {
        "id": "0bCW_tGGxtPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Not concatenating amazon train, just loading upon training because it keeps causing colab system timeout"
      ],
      "metadata": {
        "id": "EX2BP5PVyYdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Valid Amazon"
      ],
      "metadata": {
        "id": "pFY4YfOqy4a_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "print(\"EXTRACTING AMAZON VAL\")\n",
        "\n",
        "batch_size = 32\n",
        "max_length = 128\n",
        "\n",
        "def encode_and_convert_to_tensors(texts):\n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    return encoded\n",
        "\n",
        "valid_texts = valid['content'].tolist()\n",
        "num_val_examples = len(valid_texts)\n",
        "print(f\"Val samples: {num_val_examples:,}\")\n",
        "\n",
        "# Extract embeddings\n",
        "val_all_outputs_0 = []\n",
        "val_all_outputs_1 = []\n",
        "\n",
        "for i in range(0, num_val_examples, batch_size):\n",
        "    val_batch_texts = valid_texts[i:i + batch_size]\n",
        "    val_batch_inputs = encode_and_convert_to_tensors(val_batch_texts)\n",
        "    val_batch_outputs = bert_model(val_batch_inputs)\n",
        "\n",
        "    val_all_outputs_0.append(val_batch_outputs[0].numpy())\n",
        "    val_all_outputs_1.append(val_batch_outputs[1].numpy())\n",
        "\n",
        "    del val_batch_inputs, val_batch_outputs\n",
        "\n",
        "    if i % (batch_size * 50) == 0:\n",
        "        gc.collect()\n",
        "        progress = (i / num_val_examples) * 100\n",
        "        print(f\"  {progress:.1f}%\")\n",
        "\n",
        "print(\"100.0% - Extraction complete!\")\n",
        "\n",
        "# SAVE IMMEDIATELY AS PICKLE\n",
        "print(\"\\nSaving lists directly to disk...\")\n",
        "with open('/content/val_local_list.pkl', 'wb') as f:\n",
        "    pickle.dump(val_all_outputs_0, f)\n",
        "print(\"val_local_list.pkl saved\")\n",
        "\n",
        "with open('/content/val_global_list.pkl', 'wb') as f:\n",
        "    pickle.dump(val_all_outputs_1, f)\n",
        "print(\"val_global_list.pkl saved\")\n",
        "\n",
        "# DELETE FROM MEMORY\n",
        "del val_all_outputs_0, val_all_outputs_1, valid_texts\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\nVAL COMPLETE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDcmMqfry5_j",
        "outputId": "307fde46-8311-4c80-de91-723bc6ea8baa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXTRACTING AMAZON VAL\n",
            "Val samples: 11,250\n",
            "  0.0%\n",
            "  14.2%\n",
            "  28.4%\n",
            "  42.7%\n",
            "  56.9%\n",
            "  71.1%\n",
            "  85.3%\n",
            "  99.6%\n",
            "100.0% - Extraction complete!\n",
            "\n",
            "Saving lists directly to disk...\n",
            "val_local_list.pkl saved\n",
            "val_global_list.pkl saved\n",
            "\n",
            "VAL COMPLETE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concatenate Valid Amazon"
      ],
      "metadata": {
        "id": "qMAR5Ikcy7hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading\")\n",
        "\n",
        "with open('/content/val_local_list.pkl', 'rb') as f:\n",
        "    val_local_list = pickle.load(f)\n",
        "\n",
        "with open('/content/val_global_list.pkl', 'rb') as f:\n",
        "    val_global_list = pickle.load(f)\n",
        "\n",
        "print(\"Concatenating...\")\n",
        "val_local_output = np.concatenate(val_local_list, axis=0)\n",
        "val_gobal_output = np.concatenate(val_global_list, axis=0)\n",
        "\n",
        "del val_local_list, val_global_list\n",
        "\n",
        "print(f\"Val local: {val_local_output.shape}\")\n",
        "print(f\"Val global: {val_gobal_output.shape}\")\n",
        "\n",
        "# Save\n",
        "np.save('/content/val_local_output.npy', val_local_output)\n",
        "np.save('/content/val_gobal_output.npy', val_gobal_output)\n",
        "print(\"Saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20Fw5SKNy8uO",
        "outputId": "89d96e83-5c60-4168-9112-7ed70cc30fe1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading\n",
            "Concatenating...\n",
            "Val local: (11250, 128, 768)\n",
            "Val global: (11250, 768)\n",
            "Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Amazon"
      ],
      "metadata": {
        "id": "7e23Uarsy_1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EXTRACTING AMAZON TEST\")\n",
        "\n",
        "batch_size = 32\n",
        "max_length = 128\n",
        "\n",
        "def encode_and_convert_to_tensors(texts):\n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    return encoded\n",
        "\n",
        "test_texts = test['content'].tolist()\n",
        "num_test_examples = len(test_texts)\n",
        "print(f\"Test samples: {num_test_examples:,}\")\n",
        "\n",
        "# Extract embeddings\n",
        "test_all_outputs_0 = []\n",
        "test_all_outputs_1 = []\n",
        "\n",
        "for i in range(0, num_test_examples, batch_size):\n",
        "    test_batch_texts = test_texts[i:i + batch_size]\n",
        "    test_batch_inputs = encode_and_convert_to_tensors(test_batch_texts)\n",
        "    test_batch_outputs = bert_model(test_batch_inputs)\n",
        "\n",
        "    test_all_outputs_0.append(test_batch_outputs[0].numpy())\n",
        "    test_all_outputs_1.append(test_batch_outputs[1].numpy())\n",
        "\n",
        "    del test_batch_inputs, test_batch_outputs\n",
        "\n",
        "    if i % (batch_size * 50) == 0:\n",
        "        gc.collect()\n",
        "        progress = (i / num_test_examples) * 100\n",
        "        print(f\"  {progress:.1f}%\")\n",
        "\n",
        "print(\"100.0% - Extraction complete!\")\n",
        "\n",
        "# SAVE IMMEDIATELY AS PICKLE\n",
        "print(\"\\nSaving lists directly to disk...\")\n",
        "with open('/content/test_local_list.pkl', 'wb') as f:\n",
        "    pickle.dump(test_all_outputs_0, f)\n",
        "print(\"test_local_list.pkl saved\")\n",
        "\n",
        "with open('/content/test_global_list.pkl', 'wb') as f:\n",
        "    pickle.dump(test_all_outputs_1, f)\n",
        "print(\"test_global_list.pkl saved\")\n",
        "\n",
        "# DELETE FROM MEMORY\n",
        "del test_all_outputs_0, test_all_outputs_1, test_texts\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\nTEST COMPLETE \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "006j3NWVzBjO",
        "outputId": "5b09789f-c877-4a52-83de-48768f2f6ad5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EXTRACTING AMAZON TEST\n",
            "Test samples: 11,250\n",
            "  0.0%\n",
            "  14.2%\n",
            "  28.4%\n",
            "  42.7%\n",
            "  56.9%\n",
            "  71.1%\n",
            "  85.3%\n",
            "  99.6%\n",
            "100.0% - Extraction complete!\n",
            "\n",
            "Saving lists directly to disk...\n",
            "test_local_list.pkl saved\n",
            "test_global_list.pkl saved\n",
            "\n",
            "TEST COMPLETE \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concatenate Test Amazon"
      ],
      "metadata": {
        "id": "X6FRLI5hzCfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading\")\n",
        "\n",
        "with open('/content/test_local_list.pkl', 'rb') as f:\n",
        "    test_local_list = pickle.load(f)\n",
        "\n",
        "with open('/content/test_global_list.pkl', 'rb') as f:\n",
        "    test_global_list = pickle.load(f)\n",
        "\n",
        "print(\"Concatenating...\")\n",
        "test_local_output = np.concatenate(test_local_list, axis=0)\n",
        "test_gobal_output = np.concatenate(test_global_list, axis=0)\n",
        "\n",
        "del test_local_list, test_global_list\n",
        "\n",
        "print(f\"Test local: {test_local_output.shape}\")\n",
        "print(f\"Test global: {test_gobal_output.shape}\")\n",
        "\n",
        "# Save\n",
        "np.save('/content/test_local_output.npy', test_local_output)\n",
        "np.save('/content/test_gobal_output.npy', test_gobal_output)\n",
        "print(\"Saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiyWr2cJzEsr",
        "outputId": "a17c0ed6-8202-46c6-d324-97d17d98f6f6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading\n",
            "Concatenating...\n",
            "Test local: (11250, 128, 768)\n",
            "Test global: (11250, 768)\n",
            "Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yelp"
      ],
      "metadata": {
        "id": "2k7QadyqzHx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVvvmsQsPY37",
        "outputId": "3f4b90fe-198b-4bfd-a131-56c351ab6b83"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Extracting Yelp TEST - Chunked\")\n",
        "\n",
        "batch_size = 32\n",
        "max_length = 128\n",
        "CHUNK_SIZE = 20000\n",
        "\n",
        "def encode_and_convert_to_tensors(texts):\n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    return encoded\n",
        "\n",
        "yelp_texts = yelp['content'].tolist()\n",
        "num_yelp_examples = len(yelp_texts)\n",
        "\n",
        "yelp_all_outputs_0 = []\n",
        "yelp_all_outputs_1 = []\n",
        "chunk_num = 0\n",
        "samples_in_chunk = 0\n",
        "\n",
        "for i in range(0, num_yelp_examples, batch_size):\n",
        "    yelp_batch_texts = yelp_texts[i:i + batch_size]\n",
        "    yelp_batch_inputs = encode_and_convert_to_tensors(yelp_batch_texts)\n",
        "    yelp_batch_outputs = bert_model(yelp_batch_inputs)\n",
        "\n",
        "    yelp_all_outputs_0.append(yelp_batch_outputs[0].numpy())\n",
        "    yelp_all_outputs_1.append(yelp_batch_outputs[1].numpy())\n",
        "    samples_in_chunk += len(yelp_batch_texts)\n",
        "\n",
        "    del yelp_batch_inputs, yelp_batch_outputs\n",
        "\n",
        "    if samples_in_chunk >= CHUNK_SIZE or i + batch_size >= num_yelp_examples:\n",
        "        local_chunk = np.concatenate(yelp_all_outputs_0, axis=0)\n",
        "        global_chunk = np.concatenate(yelp_all_outputs_1, axis=0)\n",
        "\n",
        "        np.save(f'/content/yelp_local_chunk_{chunk_num}.npy', local_chunk)\n",
        "        np.save(f'/content/yelp_global_chunk_{chunk_num}.npy', global_chunk)\n",
        "\n",
        "        progress = ((i + batch_size) / num_yelp_examples) * 100\n",
        "        print(f\"  {progress:.1f}% - Chunk {chunk_num} saved\")\n",
        "\n",
        "        del yelp_all_outputs_0, yelp_all_outputs_1, local_chunk, global_chunk\n",
        "        yelp_all_outputs_0 = []\n",
        "        yelp_all_outputs_1 = []\n",
        "        chunk_num += 1\n",
        "        samples_in_chunk = 0\n",
        "        gc.collect()\n",
        "\n",
        "del yelp_texts\n",
        "print(\"Complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L6J17LazJTO",
        "outputId": "ac722bbc-ab39-476d-faf9-26a2321bc353"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Yelp TEST - Chunked\n",
            "  29.7% - Chunk 0 saved\n",
            "  59.4% - Chunk 1 saved\n",
            "  89.0% - Chunk 2 saved\n",
            "  100.0% - Chunk 3 saved\n",
            "Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/train_labels.npy', train_labels)\n",
        "np.save('/content/valid_labels.npy', valid_labels)\n",
        "np.save('/content/test_labels.npy', test_labels)\n",
        "np.save('/content/yelp_labels.npy', yelp_labels)"
      ],
      "metadata": {
        "id": "6TEybsMl4EvQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### verify files are ready"
      ],
      "metadata": {
        "id": "Yvkk4In5bRbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that all required files exist\n",
        "import os\n",
        "import glob\n",
        "\n",
        "required_files = {\n",
        "    'Train local chunks': '/content/train_local_chunk_*.npy',\n",
        "    'Train global': '/content/train_global_output.npy',\n",
        "    'Val local': '/content/val_local_output.npy',\n",
        "    'Val global': '/content/val_gobal_output.npy',\n",
        "    'Test local': '/content/test_local_output.npy',\n",
        "    'Test global': '/content/test_gobal_output.npy',\n",
        "    'Yelp local chunks': '/content/yelp_local_chunk_*.npy',\n",
        "    'Yelp global chunks': '/content/yelp_global_chunk_*.npy',\n",
        "    'Train labels': '/content/train_labels.npy',\n",
        "    'Val labels': '/content/valid_labels.npy',\n",
        "    'Test labels': '/content/test_labels.npy',\n",
        "    'Yelp labels': '/content/yelp_labels.npy'\n",
        "}\n",
        "\n",
        "print(\"Checking for required files...\")\n",
        "for name, pattern in required_files.items():\n",
        "    if '*' in pattern:\n",
        "        files = glob.glob(pattern)\n",
        "        if files:\n",
        "            print(f\"{name}: Found {len(files)} files\")\n",
        "        else:\n",
        "            print(f\"{name}: NOT FOUND\")\n",
        "    else:\n",
        "        if os.path.exists(pattern):\n",
        "            print(f\"{name}: Found\")\n",
        "        else:\n",
        "            print(f\"{name}: NOT FOUND\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2UtAmERbQzn",
        "outputId": "48d5dfe7-d397-462d-dd46-39a56a283d54"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for required files...\n",
            "Train local chunks: Found 3 files\n",
            "Train global: Found\n",
            "Val local: Found\n",
            "Val global: Found\n",
            "Test local: Found\n",
            "Test global: Found\n",
            "Yelp local chunks: Found 4 files\n",
            "Yelp global chunks: Found 4 files\n",
            "Train labels: Found\n",
            "Val labels: Found\n",
            "Test labels: Found\n",
            "Yelp labels: Found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Ablate Results"
      ],
      "metadata": {
        "id": "GsZVvPrabpt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Download files one category at a time\n",
        "print(\"Downloading files in batches...\")\n",
        "\n",
        "# Batch 1: Labels (small, fast)\n",
        "print(\"\\n=== Downloading Labels ===\")\n",
        "for f in ['/content/train_labels.npy', '/content/valid_labels.npy',\n",
        "          '/content/test_labels.npy', '/content/yelp_labels.npy']:\n",
        "    print(f\"Downloading {os.path.basename(f)}...\")\n",
        "    files.download(f)\n",
        "\n",
        "# Batch 2: Train global (small)\n",
        "print(\"\\n=== Downloading Train Global ===\")\n",
        "files.download('/content/train_global_output.npy')\n",
        "\n",
        "# Batch 3: Val/Test (Amazon - medium size)\n",
        "print(\"\\n=== Downloading Val/Test ===\")\n",
        "for f in ['/content/val_local_output.npy', '/content/val_gobal_output.npy',\n",
        "          '/content/test_local_output.npy', '/content/test_gobal_output.npy']:\n",
        "    print(f\"Downloading {os.path.basename(f)}...\")\n",
        "    files.download(f)\n",
        "\n",
        "# Batch 4: Train local chunks (large, one by one)\n",
        "print(\"\\n=== Downloading Train Local Chunks ===\")\n",
        "train_chunks = sorted(glob.glob('/content/train_local_chunk_*.npy'))\n",
        "for i, f in enumerate(train_chunks, 1):\n",
        "    print(f\"Downloading chunk {i}/{len(train_chunks)}: {os.path.basename(f)}...\")\n",
        "    files.download(f)\n",
        "\n",
        "# Batch 5: Yelp chunks (large, one by one)\n",
        "print(\"\\n=== Downloading Yelp Local Chunks ===\")\n",
        "yelp_local_chunks = sorted(glob.glob('/content/yelp_local_chunk_*.npy'))\n",
        "for i, f in enumerate(yelp_local_chunks, 1):\n",
        "    print(f\"Downloading chunk {i}/{len(yelp_local_chunks)}: {os.path.basename(f)}...\")\n",
        "    files.download(f)\n",
        "\n",
        "print(\"\\n=== Downloading Yelp Global Chunks ===\")\n",
        "yelp_global_chunks = sorted(glob.glob('/content/yelp_global_chunk_*.npy'))\n",
        "for i, f in enumerate(yelp_global_chunks, 1):\n",
        "    print(f\"Downloading chunk {i}/{len(yelp_global_chunks)}: {os.path.basename(f)}...\")\n",
        "    files.download(f)\n",
        "\n",
        "print(\"\\n✓ ALL DOWNLOADS COMPLETE!\")\n",
        "print(\"Create a folder called 'fake_review_data' and put all files there\")\n"
      ],
      "metadata": {
        "id": "K8BSWKfpbo_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline & Ablations"
      ],
      "metadata": {
        "id": "ziXbEi_X0aMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/content/drive/MyDrive/ablation_results', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/ablation_checkpoints', exist_ok=True)"
      ],
      "metadata": {
        "id": "8CgDLcgA9L5x"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run the cell below every time before running the ours.py script"
      ],
      "metadata": {
        "id": "FFvTrL0n9rPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import tensorflow as tf\n",
        "\n",
        "# Clear Keras session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Delete variables if they exist\n",
        "try:\n",
        "    del data, model, train_local_output, train_gobal_output, val_local_output, val_gobal_output\n",
        "    del test_local_output, test_gobal_output, yelp_local_output, yelp_gobal_output\n",
        "    del train_labels, val_labels, test_labels, yelp_labels\n",
        "    del baseline_results\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "print(\"Memory cleared\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dldovsPY9qgk",
        "outputId": "e8ce9079-ecc4-42a2-c330-cea64c14b901"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory cleared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also for some reason the yelp labels are flipped?? Doing a quick fix here and will fix properly later"
      ],
      "metadata": {
        "id": "YFzWgQC-DKZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load and flip Yelp labels\n",
        "yelp_labels = np.load('/content/yelp_labels.npy')\n",
        "print(f\"Original shape: {yelp_labels.shape}\")\n",
        "print(f\"Original first 5 rows:\\n{yelp_labels[:5]}\")\n",
        "\n",
        "# Flip the columns\n",
        "yelp_labels_fixed = yelp_labels[:, ::-1]\n",
        "\n",
        "print(f\"\\nFlipped first 5 rows:\\n{yelp_labels_fixed[:5]}\")\n",
        "\n",
        "# Save the fixed version\n",
        "np.save('/content/yelp_labels.npy', yelp_labels_fixed)\n",
        "\n",
        "# Also save to Drive as backup\n",
        "np.save('/content/drive/MyDrive/yelp_labels_FIXED.npy', yelp_labels_fixed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JRZEnv2DP1l",
        "outputId": "3f0db9c6-dd50-4e05-9483-8757bf0812d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (67395, 2)\n",
            "Original first 5 rows:\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "\n",
            "Flipped first 5 rows:\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Fake Review Detection: Baseline Training and Feature Ablation Analysis\n",
        "Adapted from Bai(2024) with some changes made to avoid colab's 90s timeout,\n",
        " uses our data, and adapted to do ablation studies.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import gc\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive for saving checkpoints and results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class Config:\n",
        "    # Data paths - Colab\n",
        "    TRAIN_LOCAL_CHUNKS = '/content/train_local_chunk_*.npy'\n",
        "    TRAIN_GLOBAL = '/content/train_global_output.npy'\n",
        "    VAL_LOCAL = '/content/val_local_output.npy'\n",
        "    VAL_GLOBAL = '/content/val_gobal_output.npy'\n",
        "    TEST_LOCAL = '/content/test_local_output.npy'\n",
        "    TEST_GLOBAL = '/content/test_gobal_output.npy'\n",
        "    YELP_LOCAL_CHUNKS = '/content/yelp_local_chunk_*.npy'\n",
        "    YELP_GLOBAL_CHUNKS = '/content/yelp_global_chunk_*.npy'\n",
        "\n",
        "    # Label paths\n",
        "    TRAIN_LABELS = '/content/train_labels.npy'\n",
        "    VAL_LABELS = '/content/valid_labels.npy'\n",
        "    TEST_LABELS = '/content/test_labels.npy'\n",
        "    YELP_LABELS = '/content/yelp_labels.npy'\n",
        "\n",
        "    # Model hyperparameters\n",
        "    MAX_LENGTH = 128\n",
        "    EMBEDDING_DIM_1 = 768\n",
        "    EMBEDDING_DIM_2 = 64\n",
        "    EXPAND_LENGTH = 12\n",
        "    NUM_FILTERS = 64\n",
        "    NUM_CLASSES = 2\n",
        "\n",
        "    # Training parameters\n",
        "    EPOCHS = 100\n",
        "    ES_PATIENCE = 2\n",
        "    BATCH_SIZE = 128\n",
        "\n",
        "    # Ablation settings\n",
        "    ABLATION_FEATURES = ['global', 'local', 'bilstm', 'cnn', 'attention']\n",
        "\n",
        "    # Output directory - save to Google Drive\n",
        "    RESULTS_DIR = '/content/drive/MyDrive/ablation_results'\n",
        "    CHECKPOINT_DIR = '/content/drive/MyDrive/ablation_checkpoints'\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(\n",
        "            name=\"att_weight\",\n",
        "            shape=(input_shape[-1], 1),\n",
        "            initializer=\"normal\"\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            name=\"att_bias\",\n",
        "            shape=(input_shape[1], 1),\n",
        "            initializer=\"zeros\"\n",
        "        )\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        et = tf.keras.backend.squeeze(\n",
        "            tf.keras.backend.tanh(\n",
        "                tf.keras.backend.dot(inputs, self.W) + self.b\n",
        "            ),\n",
        "            axis=-1\n",
        "        )\n",
        "        at = tf.keras.backend.softmax(et, axis=-1)\n",
        "        at = tf.keras.backend.expand_dims(at, axis=-1)\n",
        "        output = tf.keras.backend.sum(inputs * at, axis=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def check_gpu():\n",
        "    \"\"\"Check GPU availability\"\"\"\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        print(f\"GPU detected: {len(gpus)} device(s)\")\n",
        "        for gpu in gpus:\n",
        "            print(f\"  {gpu}\")\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(\"GPU memory growth enabled\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"GPU setup warning: {e}\")\n",
        "    else:\n",
        "        print(\"No GPU detected, using CPU\")\n",
        "    return len(gpus) > 0\n",
        "\n",
        "\n",
        "def verify_data_files():\n",
        "    \"\"\"Check that all required data files exist\"\"\"\n",
        "    print(\"VERIFYING DATA FILES\")\n",
        "\n",
        "    required_files = {\n",
        "        'Train local chunks': Config.TRAIN_LOCAL_CHUNKS,\n",
        "        'Train global': Config.TRAIN_GLOBAL,\n",
        "        'Val local': Config.VAL_LOCAL,\n",
        "        'Val global': Config.VAL_GLOBAL,\n",
        "        'Test local': Config.TEST_LOCAL,\n",
        "        'Test global': Config.TEST_GLOBAL,\n",
        "        'Yelp local chunks': Config.YELP_LOCAL_CHUNKS,\n",
        "        'Yelp global chunks': Config.YELP_GLOBAL_CHUNKS,\n",
        "        'Train labels': Config.TRAIN_LABELS,\n",
        "        'Val labels': Config.VAL_LABELS,\n",
        "        'Test labels': Config.TEST_LABELS,\n",
        "        'Yelp labels': Config.YELP_LABELS\n",
        "    }\n",
        "\n",
        "    all_good = True\n",
        "    for name, path in required_files.items():\n",
        "        if '*' in path:\n",
        "            files = glob.glob(path)\n",
        "            if files:\n",
        "                print(f\"{name}: Found {len(files)} files\")\n",
        "            else:\n",
        "                print(f\"{name}: NOT FOUND at {path}\")\n",
        "                all_good = False\n",
        "        else:\n",
        "            if os.path.exists(path):\n",
        "                size_mb = os.path.getsize(path) / (1024**2)\n",
        "                print(f\"{name}: Found ({size_mb:.2f} MB)\")\n",
        "            else:\n",
        "                print(f\"{name}: NOT FOUND at {path}\")\n",
        "                all_good = False\n",
        "\n",
        "\n",
        "    if all_good:\n",
        "        print(\"All data files ready\")\n",
        "    else:\n",
        "        print(\"ERROR: Some files missing\")\n",
        "\n",
        "\n",
        "    return all_good\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Load all preprocessed embeddings and labels with progress bars\"\"\"\n",
        "\n",
        "    print(\"LOADING DATA\")\n",
        "\n",
        "\n",
        "    # Train local\n",
        "    print(\"Loading train local chunks\")\n",
        "    train_local_chunks = sorted(glob.glob(Config.TRAIN_LOCAL_CHUNKS))\n",
        "    train_local_parts = []\n",
        "    for chunk_file in tqdm(train_local_chunks, desc=\"Train local\"):\n",
        "        train_local_parts.append(np.load(chunk_file))\n",
        "    train_local_output = np.concatenate(train_local_parts, axis=0)\n",
        "    del train_local_parts\n",
        "    gc.collect()\n",
        "    print(f\"Train local shape: {train_local_output.shape}\")\n",
        "\n",
        "    # Train global\n",
        "    print(\"Loading train global\")\n",
        "    train_gobal_output = np.load(Config.TRAIN_GLOBAL)\n",
        "    print(f\"Train global shape: {train_gobal_output.shape}\")\n",
        "\n",
        "    # Validation\n",
        "    print(\"Loading validation data\")\n",
        "    val_local_output = np.load(Config.VAL_LOCAL)\n",
        "    val_gobal_output = np.load(Config.VAL_GLOBAL)\n",
        "    print(f\"Val local shape: {val_local_output.shape}\")\n",
        "    print(f\"Val global shape: {val_gobal_output.shape}\")\n",
        "\n",
        "    # Test Amazon\n",
        "    print(\"Loading Amazon test data\")\n",
        "    test_local_output = np.load(Config.TEST_LOCAL)\n",
        "    test_gobal_output = np.load(Config.TEST_GLOBAL)\n",
        "    print(f\"Test local shape: {test_local_output.shape}\")\n",
        "    print(f\"Test global shape: {test_gobal_output.shape}\")\n",
        "\n",
        "    # Yelp\n",
        "    print(\"Loading Yelp test chunks\")\n",
        "    yelp_local_chunks = sorted(glob.glob(Config.YELP_LOCAL_CHUNKS))\n",
        "    yelp_global_chunks = sorted(glob.glob(Config.YELP_GLOBAL_CHUNKS))\n",
        "\n",
        "    yelp_local_parts = []\n",
        "    for chunk_file in tqdm(yelp_local_chunks, desc=\"Yelp local\"):\n",
        "        yelp_local_parts.append(np.load(chunk_file))\n",
        "    yelp_local_output = np.concatenate(yelp_local_parts, axis=0)\n",
        "    del yelp_local_parts\n",
        "\n",
        "    yelp_global_parts = []\n",
        "    for chunk_file in tqdm(yelp_global_chunks, desc=\"Yelp global\"):\n",
        "        yelp_global_parts.append(np.load(chunk_file))\n",
        "    yelp_gobal_output = np.concatenate(yelp_global_parts, axis=0)\n",
        "    del yelp_global_parts\n",
        "\n",
        "    gc.collect()\n",
        "    print(f\"Yelp local shape: {yelp_local_output.shape}\")\n",
        "    print(f\"Yelp global shape: {yelp_gobal_output.shape}\")\n",
        "\n",
        "    # Labels\n",
        "    print(\"Loading labels\")\n",
        "    train_labels = np.load(Config.TRAIN_LABELS)\n",
        "    val_labels = np.load(Config.VAL_LABELS)\n",
        "    test_labels = np.load(Config.TEST_LABELS)\n",
        "    yelp_labels = np.load(Config.YELP_LABELS)\n",
        "    print(f\"Train labels shape: {train_labels.shape}\")\n",
        "    print(f\"Val labels shape: {val_labels.shape}\")\n",
        "    print(f\"Test labels shape: {test_labels.shape}\")\n",
        "    print(f\"Yelp labels shape: {yelp_labels.shape}\")\n",
        "\n",
        "    # Reshape global outputs\n",
        "    print(\"Reshaping global outputs\")\n",
        "    train_gobal_output = tf.reshape(train_gobal_output, [len(train_labels), Config.EMBEDDING_DIM_2, Config.EXPAND_LENGTH])\n",
        "    val_gobal_output = tf.reshape(val_gobal_output, [len(val_labels), Config.EMBEDDING_DIM_2, Config.EXPAND_LENGTH])\n",
        "    test_gobal_output = tf.reshape(test_gobal_output, [len(test_labels), Config.EMBEDDING_DIM_2, Config.EXPAND_LENGTH])\n",
        "    yelp_gobal_output = tf.reshape(yelp_gobal_output, [len(yelp_labels), Config.EMBEDDING_DIM_2, Config.EXPAND_LENGTH])\n",
        "\n",
        "    print(\"DATA LOADING COMPLETE\")\n",
        "\n",
        "\n",
        "    return {\n",
        "        'train': (train_local_output, train_gobal_output, train_labels),\n",
        "        'val': (val_local_output, val_gobal_output, val_labels),\n",
        "        'test': (test_local_output, test_gobal_output, test_labels),\n",
        "        'yelp': (yelp_local_output, yelp_gobal_output, yelp_labels)\n",
        "    }\n",
        "\n",
        "\n",
        "def build_model(ablate_feature=None):\n",
        "    \"\"\"Build the RoBERTa-based multi-feature model\"\"\"\n",
        "    print(f\"Building model (ablating: {ablate_feature if ablate_feature else 'None'})\")\n",
        "\n",
        "    use_local = ablate_feature != 'local'\n",
        "    use_global = ablate_feature != 'global'\n",
        "    use_bilstm = ablate_feature != 'bilstm'\n",
        "    use_cnn = ablate_feature != 'cnn'\n",
        "    use_attention = ablate_feature != 'attention'\n",
        "\n",
        "    inputs = []\n",
        "    branches = []\n",
        "\n",
        "    # Local branch\n",
        "    if use_local:\n",
        "        input_1 = Input(shape=(Config.MAX_LENGTH, Config.EMBEDDING_DIM_1), name='local_input')\n",
        "        inputs.append(input_1)\n",
        "\n",
        "        if use_bilstm:\n",
        "            x1 = Bidirectional(LSTM(Config.NUM_FILTERS, return_sequences=True))(input_1)\n",
        "            x1 = Dropout(0.2)(x1)\n",
        "        else:\n",
        "            x1 = Flatten()(input_1)\n",
        "            x1 = Dense(Config.NUM_FILTERS * 2, activation='relu')(x1)\n",
        "            x1 = Dropout(0.2)(x1)\n",
        "            x1 = Reshape((1, Config.NUM_FILTERS * 2))(x1)\n",
        "\n",
        "        if use_attention and use_bilstm:\n",
        "            x1 = AttentionLayer()(x1)\n",
        "        elif not use_attention and use_bilstm:\n",
        "            x1 = GlobalMaxPooling1D()(x1)\n",
        "        else:\n",
        "            x1 = Flatten()(x1)\n",
        "\n",
        "        branches.append(x1)\n",
        "\n",
        "    # Global branch\n",
        "    if use_global:\n",
        "        input_2 = Input(shape=(Config.EMBEDDING_DIM_2, Config.EXPAND_LENGTH), name='global_input')\n",
        "        inputs.append(input_2)\n",
        "\n",
        "        if use_cnn:\n",
        "            x2 = Conv1D(32, 5, activation='relu')(input_2)\n",
        "            x2 = MaxPool1D(pool_size=2)(x2)\n",
        "            x2 = Conv1D(64, 6, activation='relu')(x2)\n",
        "            x2 = MaxPool1D(pool_size=2)(x2)\n",
        "            x2 = Dropout(0.1)(x2)\n",
        "        else:\n",
        "            x2 = Flatten()(input_2)\n",
        "            x2 = Dense(64, activation='relu')(x2)\n",
        "            x2 = Dropout(0.1)(x2)\n",
        "            x2 = Reshape((1, 64))(x2)\n",
        "\n",
        "        if use_attention and use_cnn:\n",
        "            x2 = AttentionLayer()(x2)\n",
        "        elif not use_attention and use_cnn:\n",
        "            x2 = GlobalMaxPooling1D()(x2)\n",
        "        else:\n",
        "            x2 = Flatten()(x2)\n",
        "\n",
        "        branches.append(x2)\n",
        "\n",
        "    if len(branches) == 0:\n",
        "        raise ValueError(\"Cannot ablate all features\")\n",
        "\n",
        "    # Fusion\n",
        "    if len(branches) > 1:\n",
        "        concatenated = Concatenate()(branches)\n",
        "    else:\n",
        "        concatenated = branches[0]\n",
        "\n",
        "    # Classification head\n",
        "    dense = Dense(32, activation='relu')(concatenated)\n",
        "    dense = Dropout(0.3)(dense)\n",
        "    output = Dense(Config.NUM_CLASSES, activation='softmax')(dense)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(\"Model built successfully\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def prepare_inputs(local_data, global_data, ablate_feature=None):\n",
        "    \"\"\"Prepare model inputs based on ablation configuration\"\"\"\n",
        "    use_local = ablate_feature != 'local'\n",
        "    use_global = ablate_feature != 'global'\n",
        "\n",
        "    inputs = []\n",
        "    if use_local:\n",
        "        inputs.append(local_data)\n",
        "    if use_global:\n",
        "        inputs.append(global_data)\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def train_model(model, data, ablate_feature=None, experiment_name='baseline'):\n",
        "    \"\"\"Train the model with checkpointing\"\"\"\n",
        "    train_local, train_global, train_labels = data['train']\n",
        "    val_local, val_global, val_labels = data['val']\n",
        "\n",
        "    train_inputs = prepare_inputs(train_local, train_global, ablate_feature)\n",
        "    val_inputs = prepare_inputs(val_local, val_global, ablate_feature)\n",
        "\n",
        "    print(f\"Training model: {experiment_name}\")\n",
        "    print(f\"Train samples: {len(train_labels):,}\")\n",
        "    print(f\"Val samples: {len(val_labels):,}\")\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
        "    checkpoint_path = os.path.join(Config.CHECKPOINT_DIR, f'{experiment_name}_epoch_{{epoch:02d}}_val_acc_{{val_accuracy:.4f}}.keras')\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            patience=Config.ES_PATIENCE,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=checkpoint_path,\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            mode='max',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        train_inputs,\n",
        "        train_labels,\n",
        "        epochs=Config.EPOCHS,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        validation_data=(val_inputs, val_labels),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"Training complete\")\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate_model(model, data, dataset_name, ablate_feature=None):\n",
        "    \"\"\"Evaluate model on a dataset\"\"\"\n",
        "    local_data, global_data, labels = data\n",
        "    inputs = prepare_inputs(local_data, global_data, ablate_feature)\n",
        "\n",
        "    print(f\"Evaluating on {dataset_name}\")\n",
        "    loss, accuracy = model.evaluate(inputs, labels, verbose=0)\n",
        "\n",
        "    predictions = model.predict(inputs, verbose=0)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    true_labels = np.argmax(labels, axis=1)\n",
        "\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    report = classification_report(\n",
        "        true_labels,\n",
        "        predicted_labels,\n",
        "        target_names=['Real (0)', 'Fake (1)'],\n",
        "        digits=5,\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "    results = {\n",
        "        'dataset': dataset_name,\n",
        "        'ablated_feature': ablate_feature if ablate_feature else 'None',\n",
        "        'loss': float(loss),\n",
        "        'accuracy': float(accuracy),\n",
        "        'classification_report': report,\n",
        "        'confusion_matrix': cm.tolist()\n",
        "    }\n",
        "\n",
        "    print(f\"{dataset_name} - Loss: {loss:.5f}, Accuracy: {accuracy:.5f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_results(results, filename):\n",
        "    \"\"\"Save results to JSON file\"\"\"\n",
        "    os.makedirs(Config.RESULTS_DIR, exist_ok=True)\n",
        "    filepath = os.path.join(Config.RESULTS_DIR, filename)\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"Results saved to {filepath}\")\n",
        "\n",
        "\n",
        "def print_summary(all_results):\n",
        "    \"\"\"Print summary table\"\"\"\n",
        "\n",
        "    print(\"EXPERIMENT RESULTS SUMMARY\")\n",
        "\n",
        "    print(f\"{'Model':<25} {'Amazon Acc':<15} {'Yelp Acc':<15} {'Impact Score':<15}\")\n",
        "\n",
        "\n",
        "    baseline = None\n",
        "    for result in all_results:\n",
        "        if result['ablated_feature'] == 'None':\n",
        "            baseline = result\n",
        "            break\n",
        "\n",
        "    if baseline:\n",
        "        baseline_yelp_acc = baseline['yelp_accuracy']\n",
        "\n",
        "        for result in all_results:\n",
        "            model_name = f\"Ablate: {result['ablated_feature']}\"\n",
        "            amazon_acc = result['amazon_accuracy']\n",
        "            yelp_acc = result['yelp_accuracy']\n",
        "\n",
        "            if result['ablated_feature'] == 'None':\n",
        "                impact_score = 0.0\n",
        "            else:\n",
        "                impact_score = baseline_yelp_acc - yelp_acc\n",
        "\n",
        "            print(f\"{model_name:<25} {amazon_acc:<15.5f} {yelp_acc:<15.5f} {impact_score:<15.5f}\")\n",
        "\n",
        "\n",
        "    print(\"Impact Score = Baseline Yelp Accuracy - Ablation Yelp Accuracy\")\n",
        "    print(\"Higher impact score means feature is more critical for cross-domain transfer\")\n",
        "\n",
        "\n",
        "\n",
        "def run_baseline_experiment(data):\n",
        "    \"\"\"Run baseline experiment\"\"\"\n",
        "\n",
        "    print(\"BASELINE EXPERIMENT - Full Model\")\n",
        "\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    model = build_model(ablate_feature=None)\n",
        "    history = train_model(model, data, ablate_feature=None, experiment_name='baseline')\n",
        "\n",
        "    amazon_results = evaluate_model(model, data['test'], 'Amazon Test', ablate_feature=None)\n",
        "    yelp_results = evaluate_model(model, data['yelp'], 'Yelp Test', ablate_feature=None)\n",
        "\n",
        "    model.save(os.path.join(Config.RESULTS_DIR, 'baseline_model.keras'))\n",
        "\n",
        "    results = {\n",
        "        'experiment': 'baseline',\n",
        "        'ablated_feature': 'None',\n",
        "        'amazon_accuracy': amazon_results['accuracy'],\n",
        "        'amazon_results': amazon_results,\n",
        "        'yelp_accuracy': yelp_results['accuracy'],\n",
        "        'yelp_results': yelp_results,\n",
        "        'training_history': {\n",
        "            'loss': [float(x) for x in history.history['loss']],\n",
        "            'accuracy': [float(x) for x in history.history['accuracy']],\n",
        "            'val_loss': [float(x) for x in history.history['val_loss']],\n",
        "            'val_accuracy': [float(x) for x in history.history['val_accuracy']]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    save_results(results, 'baseline_results.json')\n",
        "\n",
        "    del model\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_ablation_experiment(data, feature_to_ablate):\n",
        "    \"\"\"Run single ablation experiment\"\"\"\n",
        "\n",
        "    print(f\"ABLATION EXPERIMENT - Removing {feature_to_ablate.upper()}\")\n",
        "\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    model = build_model(ablate_feature=feature_to_ablate)\n",
        "    history = train_model(model, data, ablate_feature=feature_to_ablate, experiment_name=f'ablation_{feature_to_ablate}')\n",
        "\n",
        "    amazon_results = evaluate_model(model, data['test'], 'Amazon Test', ablate_feature=feature_to_ablate)\n",
        "    yelp_results = evaluate_model(model, data['yelp'], 'Yelp Test', ablate_feature=feature_to_ablate)\n",
        "\n",
        "    model.save(os.path.join(Config.RESULTS_DIR, f'ablation_{feature_to_ablate}_model.keras'))\n",
        "\n",
        "    results = {\n",
        "        'experiment': 'ablation',\n",
        "        'ablated_feature': feature_to_ablate,\n",
        "        'amazon_accuracy': amazon_results['accuracy'],\n",
        "        'amazon_results': amazon_results,\n",
        "        'yelp_accuracy': yelp_results['accuracy'],\n",
        "        'yelp_results': yelp_results,\n",
        "        'training_history': {\n",
        "            'loss': [float(x) for x in history.history['loss']],\n",
        "            'accuracy': [float(x) for x in history.history['accuracy']],\n",
        "            'val_loss': [float(x) for x in history.history['val_loss']],\n",
        "            'val_accuracy': [float(x) for x in history.history['val_accuracy']]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    save_results(results, f'ablation_{feature_to_ablate}_results.json')\n",
        "\n",
        "    del model\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    print(\"FAKE REVIEW DETECTION: BASELINE AND ABLATION ANALYSIS\")\n",
        "    print(\"COLAB VERSION\")\n",
        "\n",
        "    print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "\n",
        "    # Check GPU\n",
        "    has_gpu = check_gpu()\n",
        "\n",
        "    # Verify data\n",
        "    if not verify_data_files():\n",
        "        print(\"ERROR: Cannot proceed without data files\")\n",
        "        return\n",
        "\n",
        "    # Load data\n",
        "    data = load_data()\n",
        "\n",
        "    # Run baseline\n",
        "    baseline_results = run_baseline_experiment(data)\n",
        "\n",
        "    # Run ablations\n",
        "    ablation_results = []\n",
        "    for feature in Config.ABLATION_FEATURES:\n",
        "        result = run_ablation_experiment(data, feature)\n",
        "        ablation_results.append(result)\n",
        "\n",
        "    # Compile all results\n",
        "    all_results = [baseline_results] + ablation_results\n",
        "    save_results(all_results, 'all_results.json')\n",
        "\n",
        "    # Print summary\n",
        "    print_summary(all_results)\n",
        "\n",
        "\n",
        "    print(\"ALL EXPERIMENTS COMPLETE\")\n",
        "    print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Results saved to: {Config.RESULTS_DIR}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vyrWhPqMlV0",
        "outputId": "c93977dc-b681-4e19-e30f-657c3c9ac4a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "FAKE REVIEW DETECTION: BASELINE AND ABLATION ANALYSIS\n",
            "COLAB VERSION\n",
            "Start time: 2025-12-16 03:12:55\n",
            "GPU detected: 1 device(s)\n",
            "  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "GPU memory growth enabled\n",
            "VERIFYING DATA FILES\n",
            "Train local chunks: Found 3 files\n",
            "Train global: Found (153.81 MB)\n",
            "Val local: Found (4218.75 MB)\n",
            "Val global: Found (32.96 MB)\n",
            "Test local: Found (4218.75 MB)\n",
            "Test global: Found (32.96 MB)\n",
            "Yelp local chunks: Found 4 files\n",
            "Yelp global chunks: Found 4 files\n",
            "Train labels: Found (0.80 MB)\n",
            "Val labels: Found (0.17 MB)\n",
            "Test labels: Found (0.17 MB)\n",
            "Yelp labels: Found (1.03 MB)\n",
            "All data files ready\n",
            "LOADING DATA\n",
            "Loading train local chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train local: 100%|██████████| 3/3 [01:38<00:00, 32.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train local shape: (52500, 128, 768)\n",
            "Loading train global\n",
            "Train global shape: (52500, 768)\n",
            "Loading validation data\n",
            "Val local shape: (11250, 128, 768)\n",
            "Val global shape: (11250, 768)\n",
            "Loading Amazon test data\n",
            "Test local shape: (11250, 128, 768)\n",
            "Test global shape: (11250, 768)\n",
            "Loading Yelp test chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Yelp local: 100%|██████████| 4/4 [01:17<00:00, 19.50s/it]\n",
            "Yelp global: 100%|██████████| 4/4 [00:00<00:00, 53.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yelp local shape: (67395, 128, 768)\n",
            "Yelp global shape: (67395, 768)\n",
            "Loading labels\n",
            "Train labels shape: (52500, 2)\n",
            "Val labels shape: (11250, 2)\n",
            "Test labels shape: (11250, 2)\n",
            "Yelp labels shape: (67395, 2)\n",
            "Reshaping global outputs\n",
            "DATA LOADING COMPLETE\n",
            "BASELINE EXPERIMENT - Full Model\n",
            "Building model (ablating: None)\n",
            "Model built successfully\n",
            "Training model: baseline\n",
            "Train samples: 52,500\n",
            "Val samples: 11,250\n",
            "Epoch 1/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8396 - loss: 0.3699\n",
            "Epoch 1: val_accuracy improved from -inf to 0.90622, saving model to /content/drive/MyDrive/ablation_checkpoints/baseline_epoch_01_val_acc_0.9062.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 45ms/step - accuracy: 0.8400 - loss: 0.3694 - val_accuracy: 0.9062 - val_loss: 0.2321\n",
            "Epoch 2/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9156 - loss: 0.2252\n",
            "Epoch 2: val_accuracy improved from 0.90622 to 0.91271, saving model to /content/drive/MyDrive/ablation_checkpoints/baseline_epoch_02_val_acc_0.9127.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.9156 - loss: 0.2251 - val_accuracy: 0.9127 - val_loss: 0.2160\n",
            "Epoch 3/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9237 - loss: 0.2083\n",
            "Epoch 3: val_accuracy improved from 0.91271 to 0.91689, saving model to /content/drive/MyDrive/ablation_checkpoints/baseline_epoch_03_val_acc_0.9169.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.9237 - loss: 0.2083 - val_accuracy: 0.9169 - val_loss: 0.2114\n",
            "Epoch 4/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9263 - loss: 0.1988\n",
            "Epoch 4: val_accuracy improved from 0.91689 to 0.91947, saving model to /content/drive/MyDrive/ablation_checkpoints/baseline_epoch_04_val_acc_0.9195.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.9263 - loss: 0.1988 - val_accuracy: 0.9195 - val_loss: 0.2066\n",
            "Epoch 5/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9298 - loss: 0.1902\n",
            "Epoch 5: val_accuracy improved from 0.91947 to 0.91956, saving model to /content/drive/MyDrive/ablation_checkpoints/baseline_epoch_05_val_acc_0.9196.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.9299 - loss: 0.1902 - val_accuracy: 0.9196 - val_loss: 0.2068\n",
            "Epoch 6/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9355 - loss: 0.1748\n",
            "Epoch 6: val_accuracy did not improve from 0.91956\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.9355 - loss: 0.1748 - val_accuracy: 0.9192 - val_loss: 0.2050\n",
            "Epoch 7/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9389 - loss: 0.1642\n",
            "Epoch 7: val_accuracy did not improve from 0.91956\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.9389 - loss: 0.1641 - val_accuracy: 0.9167 - val_loss: 0.2213\n",
            "Epoch 8/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9443 - loss: 0.1484\n",
            "Epoch 8: val_accuracy did not improve from 0.91956\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.9443 - loss: 0.1484 - val_accuracy: 0.9148 - val_loss: 0.2306\n",
            "Epoch 8: early stopping\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "Training complete\n",
            "Evaluating on Amazon Test\n",
            "Amazon Test - Loss: 0.20349, Accuracy: 0.92258\n",
            "Evaluating on Yelp Test\n",
            "Yelp Test - Loss: 1.10434, Accuracy: 0.68278\n",
            "Results saved to /content/drive/MyDrive/ablation_results/baseline_results.json\n",
            "ABLATION EXPERIMENT - Removing GLOBAL\n",
            "Building model (ablating: global)\n",
            "Model built successfully\n",
            "Training model: ablation_global\n",
            "Train samples: 52,500\n",
            "Val samples: 11,250\n",
            "Epoch 1/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8419 - loss: 0.3643\n",
            "Epoch 1: val_accuracy improved from -inf to 0.89680, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_global_epoch_01_val_acc_0.8968.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.8422 - loss: 0.3638 - val_accuracy: 0.8968 - val_loss: 0.2518\n",
            "Epoch 2/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9134 - loss: 0.2299\n",
            "Epoch 2: val_accuracy improved from 0.89680 to 0.91271, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_global_epoch_02_val_acc_0.9127.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9135 - loss: 0.2299 - val_accuracy: 0.9127 - val_loss: 0.2172\n",
            "Epoch 3/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9221 - loss: 0.2103\n",
            "Epoch 3: val_accuracy improved from 0.91271 to 0.91547, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_global_epoch_03_val_acc_0.9155.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9221 - loss: 0.2103 - val_accuracy: 0.9155 - val_loss: 0.2125\n",
            "Epoch 4/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9256 - loss: 0.1991\n",
            "Epoch 4: val_accuracy did not improve from 0.91547\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9256 - loss: 0.1991 - val_accuracy: 0.9073 - val_loss: 0.2317\n",
            "Epoch 5/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9300 - loss: 0.1900\n",
            "Epoch 5: val_accuracy did not improve from 0.91547\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9300 - loss: 0.1900 - val_accuracy: 0.9140 - val_loss: 0.2310\n",
            "Epoch 5: early stopping\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "Training complete\n",
            "Evaluating on Amazon Test\n",
            "Amazon Test - Loss: 0.20934, Accuracy: 0.91840\n",
            "Evaluating on Yelp Test\n",
            "Yelp Test - Loss: 0.99286, Accuracy: 0.68977\n",
            "Results saved to /content/drive/MyDrive/ablation_results/ablation_global_results.json\n",
            "ABLATION EXPERIMENT - Removing LOCAL\n",
            "Building model (ablating: local)\n",
            "Model built successfully\n",
            "Training model: ablation_local\n",
            "Train samples: 52,500\n",
            "Val samples: 11,250\n",
            "Epoch 1/100\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6994 - loss: 0.6176\n",
            "Epoch 1: val_accuracy improved from -inf to 0.70000, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_local_epoch_01_val_acc_0.7000.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.6994 - loss: 0.6176 - val_accuracy: 0.7000 - val_loss: 0.6086\n",
            "Epoch 2/100\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7007 - loss: 0.6060\n",
            "Epoch 2: val_accuracy improved from 0.70000 to 0.81031, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_local_epoch_02_val_acc_0.8103.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7007 - loss: 0.6059 - val_accuracy: 0.8103 - val_loss: 0.5404\n",
            "Epoch 3/100\n",
            "\u001b[1m395/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7880 - loss: 0.4664\n",
            "Epoch 3: val_accuracy improved from 0.81031 to 0.84693, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_local_epoch_03_val_acc_0.8469.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7888 - loss: 0.4651 - val_accuracy: 0.8469 - val_loss: 0.3647\n",
            "Epoch 4/100\n",
            "\u001b[1m410/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8363 - loss: 0.3889\n",
            "Epoch 4: val_accuracy did not improve from 0.84693\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8363 - loss: 0.3889 - val_accuracy: 0.8164 - val_loss: 0.4020\n",
            "Epoch 5/100\n",
            "\u001b[1m401/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8443 - loss: 0.3725\n",
            "Epoch 5: val_accuracy did not improve from 0.84693\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8444 - loss: 0.3724 - val_accuracy: 0.8392 - val_loss: 0.3786\n",
            "Epoch 5: early stopping\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "Training complete\n",
            "Evaluating on Amazon Test\n",
            "Amazon Test - Loss: 0.36215, Accuracy: 0.84924\n",
            "Evaluating on Yelp Test\n",
            "Yelp Test - Loss: 0.66728, Accuracy: 0.68311\n",
            "Results saved to /content/drive/MyDrive/ablation_results/ablation_local_results.json\n",
            "ABLATION EXPERIMENT - Removing BILSTM\n",
            "Building model (ablating: bilstm)\n",
            "Model built successfully\n",
            "Training model: ablation_bilstm\n",
            "Train samples: 52,500\n",
            "Val samples: 11,250\n",
            "Epoch 1/100\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7607 - loss: 0.6254\n",
            "Epoch 1: val_accuracy improved from -inf to 0.88711, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_bilstm_epoch_01_val_acc_0.8871.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step - accuracy: 0.7608 - loss: 0.6249 - val_accuracy: 0.8871 - val_loss: 0.2814\n",
            "Epoch 2/100\n",
            "\u001b[1m408/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8913 - loss: 0.2788\n",
            "Epoch 2: val_accuracy did not improve from 0.88711\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8913 - loss: 0.2788 - val_accuracy: 0.8851 - val_loss: 0.2775\n",
            "Epoch 3/100\n",
            "\u001b[1m410/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9108 - loss: 0.2277\n",
            "Epoch 3: val_accuracy did not improve from 0.88711\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9108 - loss: 0.2277 - val_accuracy: 0.8804 - val_loss: 0.2879\n",
            "Epoch 4/100\n",
            "\u001b[1m408/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9212 - loss: 0.2003\n",
            "Epoch 4: val_accuracy did not improve from 0.88711\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9212 - loss: 0.2004 - val_accuracy: 0.8814 - val_loss: 0.2996\n",
            "Epoch 4: early stopping\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "Training complete\n",
            "Evaluating on Amazon Test\n",
            "Amazon Test - Loss: 0.27179, Accuracy: 0.89200\n",
            "Evaluating on Yelp Test\n",
            "Yelp Test - Loss: 0.96366, Accuracy: 0.67168\n",
            "Results saved to /content/drive/MyDrive/ablation_results/ablation_bilstm_results.json\n",
            "ABLATION EXPERIMENT - Removing CNN\n",
            "Building model (ablating: cnn)\n",
            "Model built successfully\n",
            "Training model: ablation_cnn\n",
            "Train samples: 52,500\n",
            "Val samples: 11,250\n",
            "Epoch 1/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7988 - loss: 0.4344\n",
            "Epoch 1: val_accuracy improved from -inf to 0.90622, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_cnn_epoch_01_val_acc_0.9062.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.7992 - loss: 0.4336 - val_accuracy: 0.9062 - val_loss: 0.2349\n",
            "Epoch 2/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9116 - loss: 0.2384\n",
            "Epoch 2: val_accuracy improved from 0.90622 to 0.91431, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_cnn_epoch_02_val_acc_0.9143.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9116 - loss: 0.2383 - val_accuracy: 0.9143 - val_loss: 0.2162\n",
            "Epoch 3/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9187 - loss: 0.2198\n",
            "Epoch 3: val_accuracy did not improve from 0.91431\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9187 - loss: 0.2198 - val_accuracy: 0.9044 - val_loss: 0.2329\n",
            "Epoch 4/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9231 - loss: 0.2059\n",
            "Epoch 4: val_accuracy improved from 0.91431 to 0.91538, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_cnn_epoch_04_val_acc_0.9154.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9231 - loss: 0.2059 - val_accuracy: 0.9154 - val_loss: 0.2111\n",
            "Epoch 5/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9288 - loss: 0.1934\n",
            "Epoch 5: val_accuracy improved from 0.91538 to 0.91564, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_cnn_epoch_05_val_acc_0.9156.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9288 - loss: 0.1934 - val_accuracy: 0.9156 - val_loss: 0.2080\n",
            "Epoch 6/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9334 - loss: 0.1808\n",
            "Epoch 6: val_accuracy did not improve from 0.91564\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9334 - loss: 0.1809 - val_accuracy: 0.9153 - val_loss: 0.2157\n",
            "Epoch 7/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9401 - loss: 0.1642\n",
            "Epoch 7: val_accuracy did not improve from 0.91564\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9400 - loss: 0.1643 - val_accuracy: 0.9085 - val_loss: 0.2421\n",
            "Epoch 7: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "Training complete\n",
            "Evaluating on Amazon Test\n",
            "Amazon Test - Loss: 0.20598, Accuracy: 0.92089\n",
            "Evaluating on Yelp Test\n",
            "Yelp Test - Loss: 0.98642, Accuracy: 0.69418\n",
            "Results saved to /content/drive/MyDrive/ablation_results/ablation_cnn_results.json\n",
            "ABLATION EXPERIMENT - Removing ATTENTION\n",
            "Building model (ablating: attention)\n",
            "Model built successfully\n",
            "Training model: ablation_attention\n",
            "Train samples: 52,500\n",
            "Val samples: 11,250\n",
            "Epoch 1/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8479 - loss: 0.3542\n",
            "Epoch 1: val_accuracy improved from -inf to 0.91298, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_attention_epoch_01_val_acc_0.9130.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.8482 - loss: 0.3537 - val_accuracy: 0.9130 - val_loss: 0.2242\n",
            "Epoch 2/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9179 - loss: 0.2190\n",
            "Epoch 2: val_accuracy improved from 0.91298 to 0.91520, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_attention_epoch_02_val_acc_0.9152.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9179 - loss: 0.2190 - val_accuracy: 0.9152 - val_loss: 0.2206\n",
            "Epoch 3/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9251 - loss: 0.1991\n",
            "Epoch 3: val_accuracy improved from 0.91520 to 0.91760, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_attention_epoch_03_val_acc_0.9176.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9251 - loss: 0.1991 - val_accuracy: 0.9176 - val_loss: 0.2116\n",
            "Epoch 4/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9316 - loss: 0.1843\n",
            "Epoch 4: val_accuracy improved from 0.91760 to 0.91840, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_attention_epoch_04_val_acc_0.9184.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9316 - loss: 0.1844 - val_accuracy: 0.9184 - val_loss: 0.2061\n",
            "Epoch 5/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9367 - loss: 0.1698\n",
            "Epoch 5: val_accuracy improved from 0.91840 to 0.92098, saving model to /content/drive/MyDrive/ablation_checkpoints/ablation_attention_epoch_05_val_acc_0.9210.keras\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9367 - loss: 0.1698 - val_accuracy: 0.9210 - val_loss: 0.2092\n",
            "Epoch 6/100\n",
            "\u001b[1m409/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9397 - loss: 0.1575\n",
            "Epoch 6: val_accuracy did not improve from 0.92098\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9398 - loss: 0.1575 - val_accuracy: 0.9193 - val_loss: 0.2082\n",
            "Epoch 6: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "Training complete\n",
            "Evaluating on Amazon Test\n",
            "Amazon Test - Loss: 0.20303, Accuracy: 0.92213\n",
            "Evaluating on Yelp Test\n",
            "Yelp Test - Loss: 1.09741, Accuracy: 0.66699\n",
            "Results saved to /content/drive/MyDrive/ablation_results/ablation_attention_results.json\n",
            "Results saved to /content/drive/MyDrive/ablation_results/all_results.json\n",
            "EXPERIMENT RESULTS SUMMARY\n",
            "Model                     Amazon Acc      Yelp Acc        Impact Score   \n",
            "Ablate: None              0.92258         0.68278         0.00000        \n",
            "Ablate: global            0.91840         0.68977         -0.00699       \n",
            "Ablate: local             0.84924         0.68311         -0.00033       \n",
            "Ablate: bilstm            0.89200         0.67168         0.01110        \n",
            "Ablate: cnn               0.92089         0.69418         -0.01140       \n",
            "Ablate: attention         0.92213         0.66699         0.01579        \n",
            "Impact Score = Baseline Yelp Accuracy - Ablation Yelp Accuracy\n",
            "Higher impact score means feature is more critical for cross-domain transfer\n",
            "ALL EXPERIMENTS COMPLETE\n",
            "End time: 2025-12-16 03:36:12\n",
            "Results saved to: /content/drive/MyDrive/ablation_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result Verification\n",
        "We found that not only do impact scores not differ much for each ablation study, but also that they don't even differ much from the baseline accuracy which we thought was strange. Maybe this is because other feautures compensate for the missing one, but the code below will verify that the model architectures for the ablations are acutally different."
      ],
      "metadata": {
        "id": "Y6tPIqyaTCbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test - check model architectures\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "# Load baseline\n",
        "baseline = load_model('/content/drive/MyDrive/ablation_results/baseline_model.keras',\n",
        "                     custom_objects={'AttentionLayer': AttentionLayer})\n",
        "\n",
        "# Load ablation\n",
        "ablation_local = load_model('/content/drive/MyDrive/ablation_results/ablation_local_model.keras',\n",
        "                           custom_objects={'AttentionLayer': AttentionLayer})\n",
        "\n",
        "print(\"Baseline inputs:\", len(baseline.inputs))\n",
        "print(\"Baseline trainable params:\", baseline.count_params())\n",
        "\n",
        "print(\"\\nAblation (no local) inputs:\", len(ablation_local.inputs))\n",
        "print(\"Ablation (no local) trainable params:\", ablation_local.count_params())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOZDD9sgTCKH",
        "outputId": "7a31e2d3-265f-424c-9789-a588319d2353"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline inputs: 2\n",
            "Baseline trainable params: 447374\n",
            "\n",
            "Ablation (no local) inputs: 1\n",
            "Ablation (no local) trainable params: 16526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qYxz3cLyFfDP"
      }
    }
  ]
}